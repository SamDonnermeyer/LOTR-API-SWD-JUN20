{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API Request to Get LOTR Data from LOTRAPI ## \n",
    "\n",
    "\"\"\"\n",
    "https://the-one-api.herokuapp.com/documentation \n",
    "Bearer Token - acess Key\n",
    "\n",
    "Endpoint\tResponse\tToken required\n",
    "/book\tList of all \"The Lord of the Rings\" books\tno\n",
    "/book/{id}\tRequest one specific book\tno\n",
    "/book/{id}/chapter\tRequest all chapters of one specific book\tno\n",
    "/movie\tList of all movies, including the \"The Lord of the Rings\" and the \"The Hobbit\" trilogies\tyes\n",
    "/movie/{id}\tRequest one specific movie\tyes\n",
    "/movie/{id}/quote\tRequest all movie quotes for one specific movie (only working for the LotR trilogy)\tyes\n",
    "/character\tList of characters including metadata like name, gender, realm, race and more\tyes\n",
    "/character/{id}\tRequest one specific character\tyes\n",
    "/character/{id}/quote\tRequest all movie quotes of one specific character\tyes\n",
    "/quote\tList of all movie quotes\tyes\n",
    "/quote/{id}\tRequest one specific movie quote\tyes\n",
    "/chapter\tList of all book chapters\tyes\n",
    "/chapter/{id}\tRequest one specific book chapter\tyes\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages #\n",
    "import io\n",
    "import json\n",
    "from lxml import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Variables # \n",
    "api_key = \"Bearer Key\"\n",
    "endpoint = [\"book\", \"character\",\"quote\", \"movie\"]\n",
    "api_url_base = \"https://the-one-api.herokuapp.com/v1/\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Loop to gather data for each endpoint\n",
    "for api_end in endpoint:\n",
    "    # Initialize the url\n",
    "    api_url = \"{}{}\".format(api_url_base, api_end)\n",
    "    \n",
    "    # Request API \n",
    "    r = requests.get(api_url, headers = headers)\n",
    "    \n",
    "    # Check status of request\n",
    "    if r.status_code == 200: \n",
    "        # Put the request into json format\n",
    "        r_json = r.json()\n",
    "        # Json -> df\n",
    "        lotr_df = pd.DataFrame(r_json[\"docs\"])\n",
    "        # Get quote data to be analyzed further\n",
    "        if api_end == \"quote\":\n",
    "            quote_df = lotr_df\n",
    "        # Export the dataframe to csv for visualization\n",
    "        lotr_df.to_csv(\"LOTR_{}_.csv\".format(api_end), index=False)\n",
    "    else: \n",
    "        # If the request errors. \n",
    "        print(\"ERROR: Status {} with {} endpoint.\".format(r.status_code, api_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Sentiment Analysis for each character within the fellowship ##\n",
    "\n",
    "# All of the members of the fellowship #  \n",
    "fellowship_char = [\"5cd99d4bde30eff6ebccfea0\", # Gandalf\n",
    "                   \"5cd99d4bde30eff6ebccfc15\", # Frodo\n",
    "                   \"5cd99d4bde30eff6ebccfd0d\", # Samwise\n",
    "                   \"5cd99d4bde30eff6ebccfc7c\", # Merry\n",
    "                   \"5cd99d4bde30eff6ebccfe2e\", # Pippin\n",
    "                   \"5cd99d4bde30eff6ebccfbe6\", # Aragorn\n",
    "                   \"5cd99d4bde30eff6ebccfd23\", # Gimli\n",
    "                   \"5cd99d4bde30eff6ebccfd81\", # Legolas\n",
    "                   \"5cd99d4bde30eff6ebccfc57\", # Boromir\n",
    "                  ]\n",
    "\n",
    "# Refine the df to just fellowship members\n",
    "mask = quote_df[\"character\"].isin(fellowship_char)\n",
    "fellowship_df = quote_df[~mask]\n",
    "fellowship_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start Cleaning Text ##\n",
    "\n",
    "# Create empty column to add cleaned text\n",
    "fellowship_df[\"TEXT\"] = \"\"\n",
    "\n",
    "all_words = []\n",
    "character_words = []\n",
    "\n",
    "# Create Stopwords \n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# Define Parts of Speech to Allow - J: adjective, R: Adverb, V: Verb\n",
    "allowed_word_types = [\"V\"] #[\"J\", \"R\", \"V\"]\n",
    "\n",
    "# Iterate through each row in df\n",
    "for index, row in fellowship_df.iterrows():    \n",
    "    # Remove Punctations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]', '', row[\"dialog\"])\n",
    "    \n",
    "    # Tokenize the strings\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # Remove Stopwords from strings\n",
    "    stopped = [word for word in tokenized if not word in stop_words]\n",
    "    \n",
    "    # Tag part of speech for each word\n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of all pos that passed through the filters above\n",
    "    for word in pos:\n",
    "        if word[1][0] in allowed_word_types:\n",
    "            #all_words.append(w[0].lower())\n",
    "            row[\"TEXT\"] = word[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts how many empty values there are in the TEXT column - will eliminate\n",
    "count_empty = (fellowship_df[\"TEXT\"] == \"\").sum(axis = 0)\n",
    "total_count = fellowship_df.shape[0]\n",
    "print(\"There are {} empty values out of {} records.\".format(count_empty, total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate empty values from the fellowship_df\n",
    "empty_string_mask = (fellowship_df[\"TEXT\"] == \"\")\n",
    "fellowship_df = fellowship_df[~empty_string_mask]\n",
    "\n",
    "# Export to CSV \n",
    "fellowship_df.to_csv(\"LOTR_fellowship_verbs_.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
